# Getting Started - Pentaho to DBT Migration

Complete guide to set up and run the automated Pentaho to DBT migration system.

---

## üö® MEGA-IMPORTANT RULE

**NEVER commit directly to `develop`, `master`, or `main` branches!**

The system has built-in protection to prevent this, but you should always:
- ‚úÖ Work on feature branches (`migrate/dimension_name`)
- ‚úÖ Create Pull/Merge Requests for code review
- ‚ùå NEVER commit directly to protected branches

**The `/migrate` command automatically creates feature branches for you.**

---

## Quick Start (5 Minutes)

### 1. Prerequisites

‚úÖ **Already installed:**
- Git Bash (MINGW64)
- DBT Cloud CLI (`dbt.exe` in `tfses-dbt-snowflake-3030/`)
- Snowflake connection configured

‚úÖ **Need to install:**
```bash
# GitLab CLI (if using GitLab)
winget install gitlab.glab
glab auth login

# GitHub CLI (if using GitHub)
winget install GitHub.cli
gh auth login
```

### 2. One-Time Setup (PATH Configuration)

Run this script once to make `dbt` command available everywhere:

```bash
cd 3030-pentaho-dbt
bash setup-dbt-path.sh
source ~/.bashrc
```

**Verify it worked:**
```bash
dbt --version
# Should show: dbt Cloud CLI - 0.40.7
```

### 3. Run Your First Migration

```bash
# Test with small dimension (safe, no git)
/improve dim_date

# Production migration (with git push)
/migrate dim_approval_level
```

**The system will guide you through the process** with Safe Mode prompts at each decision point.

**Done!** ‚úÖ

---

## üîí Safe Mode (NEW - Default Behavior)

**As of v3.1, the migration system runs in SAFE MODE by default.**

This means the system will pause and ask for your confirmation at critical points:

### What Safe Mode Does

‚úÖ **Unknown Variables**
- System finds similar variables and suggests mappings
- You confirm if the suggestion is correct

‚úÖ **Missing Row Counts**
- System asks if you want VIEW or TABLE materialization
- You choose based on expected data volume

‚úÖ **Custom Functions**
- System detects unknown SQL functions
- You confirm if they're custom UDFs or standard Oracle

‚úÖ **Missing Source Tables**
- System detects tables not in Snowflake
- You choose to skip, wait, or stop

‚úÖ **Step Reviews**
- After each pipeline step, system shows results
- You confirm everything looks correct before continuing

### Example Interactions

```
Variable ${UNKNOWN_SCHEMA} not found.
Found similar: ${EKIP_SCHEMA} ‚Üí EKIP (93% match). Use this?

Options:
1. Yes, use EKIP
2. No, I'll provide correct value
3. Stop, let me fix manually
```

This ensures you're always in control and prevents surprises!

---

## Understanding the System

### What It Does

Automatically converts **Pentaho transformations** (.ktr, .kjb) into **production-ready DBT models** for Snowflake:

```
Pentaho XML ‚Üí Parse ‚Üí Analyze ‚Üí Translate ‚Üí Generate ‚Üí Validate ‚Üí Git Push
```

### Two Commands

| Command | Purpose | Git Operations | Use When |
|---------|---------|----------------|----------|
| `/improve` | Test locally | ‚ùå No | Testing improvements, safe experimentation |
| `/migrate` | Production | ‚úÖ Yes | Ready to deploy to production |

---

## The Migration Workflow

### Step-by-Step Process

**1. Parse** (`pentaho-parser` skill)
```bash
Input:  pentaho-sources/dim_approval_level/*.ktr, *.kjb
Output: dimensions/dim_approval_level/metadata/pentaho_raw.json
```
Extracts SQL, variables, steps, tables from Pentaho XML.

**2. Analyze** (`pentaho-analyzer` agent)
```bash
Input:  pentaho_raw.json, schema_registry.json
Output: pentaho_analyzed.json
```
Resolves variables, classifies tables (bronze/silver/gold), assesses complexity.

**3. Build Dependencies** (`dependency-graph-builder` agent)
```bash
Input:  pentaho_raw.json, pentaho_analyzed.json
Output: dependency_graph.json, dependency_graph.mmd
```
Determines execution order, detects circular dependencies.

**4. Translate SQL** (`sql-translator` agent)
```bash
Input:  pentaho_analyzed.json, oracle-snowflake-rules
Output: *_translated.sql, translation_metadata.json
```
Converts Oracle SQL to Snowflake, preserves custom UDFs.

**5. Generate DBT Models** (`dbt-model-generator` agent)
```bash
Input:  translation_metadata.json, dbt-best-practices
Output: DBT models in models/silver/, models/gold/
```
Creates production-ready DBT models with docs and tests.

**6. Validate Locally** (`quality-validator` agent) ‚ú® **NEW!**
```bash
Runs LOCALLY (no CI/CD wait):
  ‚Ä¢ dbt parse   (syntax validation)
  ‚Ä¢ dbt compile (template validation)
  ‚Ä¢ dbt run     (create models in Snowflake)
  ‚Ä¢ dbt test    (data quality tests)

If errors: Auto-fix and retry (max 2 times)
If passes: Git commit + push
```

**Total Time:** ~3 minutes (was 10-15 min with CI/CD)

### üìã Review Screens (Safe Mode)

After each step, you'll see a review screen:

```
‚úÖ STEP 2 COMPLETE: Analysis
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

RESOLVED VARIABLES:
‚úÖ ${EKIP_SCHEMA} ‚Üí EKIP

USER-CONFIRMED:
‚úÖ ${NEW_VAR} ‚Üí VALUE (you confirmed)

Review complete. Does everything look correct?
1. ‚úÖ Yes, continue to Step 3
2. ‚ùå No, let me fix something
3. üìù Show detailed analysis
```

This ensures you can review and confirm results before proceeding.

---

## Platform Support (GitHub & GitLab)

The system **auto-detects** your Git platform from the remote URL:

### GitHub
```bash
Remote: https://github.com/org/repo.git
‚Üí Detected: GitHub
‚Üí Uses: gh CLI
‚Üí Creates: Pull Request
```

### GitLab
```bash
Remote: https://gitlab.com/org/repo.git
‚Üí Detected: GitLab
‚Üí Uses: glab CLI
‚Üí Creates: Merge Request
```

**Same `/migrate` command works for both!**

---

## Configuration Files

### schema_registry.json

Maps Pentaho variables to Snowflake schemas:

```json
{
  "variables": {
    "EKIP_SCHEMA": {
      "snowflake_name": "EKIP",
      "type": "external",
      "layer": "bronze"
    }
  },
  "custom_functions": [
    {
      "name": "GETENNUML",
      "preserve": true,
      "deployment_required": true
    }
  ]
}
```

**When to edit:**
- Adding new Pentaho variable
- Declaring custom UDF (so it's not translated)

### TABLE_COUNT.csv (Optional)

Used for materialization optimization:

```csv
schema,table,row_count
EKIP,CONTRACTS,50234
EKIP,CUSTOMERS,12500
```

**Rules:**
- `> 10M rows` ‚Üí Materialized as `table`
- `< 10M rows` ‚Üí Materialized as `view`

---

## Folder Structure

```
3030-pentaho-dbt/
‚îú‚îÄ‚îÄ CLAUDE.md                    # Context for Claude Code
‚îú‚îÄ‚îÄ README.md                    # Overview
‚îú‚îÄ‚îÄ GETTING_STARTED.md          # This file
‚îú‚îÄ‚îÄ setup-dbt-path.sh           # PATH setup script
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ schema_registry.json    # Variable mappings
‚îÇ   ‚îî‚îÄ‚îÄ TABLE_COUNT.csv         # Table sizes (optional)
‚îÇ
‚îú‚îÄ‚îÄ pentaho-sources/                    # INPUT: Pentaho source files
‚îÇ   ‚îî‚îÄ‚îÄ dim_approval_level/
‚îÇ       ‚îú‚îÄ‚îÄ d_approval_level.ktr
‚îÇ       ‚îî‚îÄ‚îÄ *.kjb
‚îÇ
‚îú‚îÄ‚îÄ dimensions/                 # OUTPUT: Metadata per dimension
‚îÇ   ‚îî‚îÄ‚îÄ dim_approval_level/
‚îÇ       ‚îú‚îÄ‚îÄ metadata/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ pentaho_raw.json
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ pentaho_analyzed.json
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dependency_graph.json
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ translation_metadata.json
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dbt_generation_report.json
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ validation_report.json
‚îÇ       ‚îî‚îÄ‚îÄ sql/
‚îÇ           ‚îî‚îÄ‚îÄ *_translated.sql
‚îÇ
‚îú‚îÄ‚îÄ tfses-dbt-snowflake-3030/  # DBT repository (git)
‚îÇ   ‚îú‚îÄ‚îÄ dbt.exe                 # DBT CLI binary
‚îÇ   ‚îú‚îÄ‚îÄ profiles.yml            # Snowflake connection
‚îÇ   ‚îú‚îÄ‚îÄ dbt_project.yml
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îú‚îÄ‚îÄ bronze/_sources.yml
‚îÇ       ‚îú‚îÄ‚îÄ silver/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ silver_adq/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ silver_mas/
‚îÇ       ‚îî‚îÄ‚îÄ gold/
‚îÇ
‚îî‚îÄ‚îÄ .claude/
    ‚îú‚îÄ‚îÄ skills/                 # Deterministic operations
    ‚îÇ   ‚îú‚îÄ‚îÄ pentaho-parser/
    ‚îÇ   ‚îú‚îÄ‚îÄ oracle-snowflake-rules/
    ‚îÇ   ‚îî‚îÄ‚îÄ dbt-best-practices/
    ‚îÇ       ‚îî‚îÄ‚îÄ reference/
    ‚îÇ           ‚îî‚îÄ‚îÄ repo_context/
    ‚îÇ               ‚îú‚îÄ‚îÄ macros.md
    ‚îÇ               ‚îú‚îÄ‚îÄ models_inventory.md
    ‚îÇ               ‚îú‚îÄ‚îÄ sources_inventory.md
    ‚îÇ               ‚îú‚îÄ‚îÄ test_patterns.md
    ‚îÇ               ‚îú‚îÄ‚îÄ project_config.md
    ‚îÇ               ‚îú‚îÄ‚îÄ lessons_learned.md       # üìö NEW: Knowledge base
    ‚îÇ               ‚îî‚îÄ‚îÄ learnings_summary.md     # üìö NEW: Current migration guidance
    ‚îú‚îÄ‚îÄ agents/                 # AI-powered analysis (12 agents)
    ‚îÇ   ‚îú‚îÄ‚îÄ pentaho-analyzer.md
    ‚îÇ   ‚îú‚îÄ‚îÄ sql-translator.md
    ‚îÇ   ‚îú‚îÄ‚îÄ dbt-model-generator.md
    ‚îÇ   ‚îú‚îÄ‚îÄ quality-validator.md
    ‚îÇ   ‚îú‚îÄ‚îÄ learning-logger.md          # üìö NEW: Processes learnings
    ‚îÇ   ‚îî‚îÄ‚îÄ ... (7 more agents)
    ‚îî‚îÄ‚îÄ commands/               # Workflow orchestration
        ‚îú‚îÄ‚îÄ migrate.md
        ‚îú‚îÄ‚îÄ improve.md
        ‚îî‚îÄ‚îÄ pause-model-migration.md
```

---

## DBT Naming Conventions

The system follows strict naming rules:

| Pentaho File | DBT Model | Layer |
|--------------|-----------|-------|
| `adq_ekip_contracts.ktr` | `silver/silver_adq/stg_ekip_contracts.sql` | Silver ADQ |
| `mas_contracts.kjb` | `silver/silver_mas/mas_contracts.sql` | Silver MAS |
| `d_approval_level.ktr` | `gold/d_approval_level.sql` | Gold (dimension) |
| `f_sales.ktr` | `gold/f_sales.sql` | Gold (fact) |

**Pattern:**
- Remove `adq_` prefix ‚Üí Add `stg_` prefix
- Keep `mas_` prefix
- Keep `d_` (dimension) and `f_` (fact) prefixes

---

## Common Tasks

### Migrate a New Dimension

```bash
# 1. Place Pentaho files
mkdir pentaho-sources/dim_customer
cp /path/to/*.ktr pentaho-sources/dim_customer/

# 2. Run migration
/migrate dim_customer

# 3. Review validation report
cat dimensions/dim_customer/metadata/validation_report.json | jq

# 4. Review Merge Request and merge
```

### Test Improvements Before Deploying

```bash
# Make changes to agents/skills
# Test without git operations
/improve dim_customer

# Compare results
diff -r tfses-dbt-snowflake-3030/models tfses-dbt-snowflake-3030-ai/models

# If good, run production
/migrate dim_customer
```

### Add New Variable Mapping

Edit `config/schema_registry.json`:

```json
{
  "variables": {
    "NEW_SCHEMA": {
      "snowflake_name": "ACTUAL_SCHEMA_NAME",
      "type": "external",
      "layer": "bronze"
    }
  }
}
```

Then re-run `/migrate`.

### Declare Custom UDF

Edit `config/schema_registry.json`:

```json
{
  "custom_functions": [
    {
      "name": "MY_CUSTOM_FUNCTION",
      "preserve": true,
      "deployment_required": true,
      "description": "Custom UDF - do not translate"
    }
  ]
}
```

**Remember:** Deploy UDF to Snowflake before running models!

---

## Troubleshooting

### "dbt: command not found"

**Cause:** PATH not set up

**Solution:**
```bash
source ~/.bashrc
# or
bash setup-dbt-path.sh
```

### "Cannot connect to Snowflake"

**Cause:** profiles.yml misconfigured

**Solution:**
```bash
cd tfses-dbt-snowflake-3030
dbt debug  # Test connection
```

Check `profiles.yml` has correct credentials.

### "Variable not found" Error

**Safe Mode Behavior:** System will suggest similar variables and ask for confirmation.

**If you choose "Stop, let me fix manually":**
Add to `config/schema_registry.json`:
```json
{
  "variables": {
    "YOUR_SCHEMA": {
      "snowflake_name": "ACTUAL_NAME",
      "type": "external",
      "layer": "bronze"
    }
  }
}
```

### "Circular dependency detected"

**Cause:** Pentaho transformations depend on each other in a loop

**Solution:**
1. Review `dimensions/{dimension}/metadata/dependency_graph.mmd`
2. Identify the cycle
3. Redesign transformation logic to break the loop
4. See `dependency_graph.json` for suggested break points

### "Source table not found in Snowflake"

**Safe Mode Behavior:** System will ask what you want to do:
- **Skip models** - Continue without models that use this table
- **Wait** - System waits 30s while you load the table
- **Stop** - Stop migration for critical tables

**To load missing tables:**
1. Copy data from Oracle to Snowflake
2. Choose "Wait" option when prompted
3. System will retry and continue

### Migration Fails with Errors

**The system will:**
1. Show you the error and ask what to do (Safe Mode)
2. Attempt auto-fix if you choose to continue (max 2 times)
3. If can't fix: Display clear error message with remediation steps
4. Fix manually
5. Re-run `/migrate {dimension}`

---

## Learning System (NEW in v3.1) üìö

**The system now learns from each migration to prevent repeated mistakes!**

### How It Works

Every time a migration encounters an issue and fixes it, the system can log it as a "learning":

```
quality-validator encounters issue ‚Üí Signals üìö LEARNING
    ‚Üì
learning-logger agent ‚Üí Processes and stores learning
    ‚Üì
repo-analyzer ‚Üí Reads learnings for next migration
    ‚Üì
All agents ‚Üí Receive proactive guidance
    ‚Üì
Future migrations ‚Üí Catch issues earlier!
```

### Example Learning

From dim_contract migration (2025-10-29):

**GETENUMML UDF Issue:**
- ‚ùå **Problem**: Function exists but returns wrong data
- ‚úÖ **Solution**: Replace with explicit JOINs
- üîç **Prevention**: sql-translator now auto-detects and replaces
- üìà **Impact**: Prevents data corruption in all future migrations

### Knowledge Base Location

`.claude/skills/dbt-best-practices/reference/repo_context/lessons_learned.md`

This file accumulates knowledge from all migrations and is automatically read by repo-analyzer.

### Benefits

- ‚úÖ **Self-improving** - System gets smarter with each migration
- ‚úÖ **Proactive** - Catches known issues before they fail
- ‚úÖ **Faster** - Less manual intervention over time
- ‚úÖ **Shared knowledge** - All agents learn from each other

See **CLAUDE.md** for detailed documentation on the learning system.

---

## Advanced Topics

### Auto-Fix Capabilities

The quality-validator agent can automatically fix:

**‚úÖ Auto-fixable:**
- Missing source definitions ‚Üí Adds to `_sources.yml`
- Common typos ‚Üí `FORM` ‚Üí `FROM`, `SLECT` ‚Üí `SELECT`

**‚ö†Ô∏è Requires manual fix:**
- Invalid model references
- Complex SQL syntax errors
- Business logic issues

**Circuit breaker:** Max 2 auto-fix attempts, then stops.

### Local vs Remote Validation

| Aspect | Local (Current) | CI/CD (Optional) |
|--------|-----------------|------------------|
| **Speed** | 30 seconds | 2-5 minutes |
| **Setup** | None (dbt already works) | GitLab CI config needed |
| **Where runs** | Your machine | GitLab runner |
| **Snowflake creds** | Your local profile | CI/CD variables |
| **When to use** | Development, single developer | Production gate, team |

**Current workflow uses LOCAL validation** (faster, simpler).

Optional CI/CD setup available in `docs/archive/GITLAB_CICD_SETUP.md`.

### Materialization Strategy

Automatically determined:

| Layer | Default | Exception |
|-------|---------|-----------|
| **silver_adq** | `view` | `table` if > 10M rows |
| **silver_mas** | `table` | (business logic layer) |
| **gold dimensions** | `table` | (small, frequently queried) |
| **gold facts** | `incremental` | (large, append-only) |

Uses `TABLE_COUNT.csv` for row count lookup.

---

## Commands Reference

### Migration Commands

```bash
/migrate {dimension}          # Full migration with git push
/improve {dimension}          # Test locally (no git)
/migration-status             # Check all dimensions
/migration-status {dimension} # Check specific dimension
```

### DBT Commands (Local)

```bash
cd tfses-dbt-snowflake-3030

# Validation
dbt parse                              # Syntax check
dbt compile                            # Template resolution
dbt debug                              # Test Snowflake connection

# Execution
dbt run                                # Run all models
dbt run --select tag:dim_customer      # Run specific dimension
dbt test                               # Run all tests
dbt test --select tag:dim_customer     # Test specific dimension

# Documentation
dbt docs generate                      # Generate docs
dbt docs serve                         # View docs in browser
```

### Git Commands (if needed)

```bash
cd tfses-dbt-snowflake-3030

# Check status
git status
git branch  # See current branch
git log --oneline -10

# Create MR/PR manually
glab mr create  # GitLab
gh pr create    # GitHub

# View MR/PR
glab mr view    # GitLab
gh pr view      # GitHub
```

### üö® Branch Safety

**CRITICAL:** The system enforces this rule automatically:

```bash
# ‚úÖ GOOD - Feature branch
git checkout -b migrate/dim_customer

# ‚ùå BAD - Protected branch (BLOCKED!)
git checkout develop  # System will abort migration
git checkout master   # System will abort migration
git checkout main     # System will abort migration
```

**Protection in place:**
1. `/migrate` command creates feature branch automatically
2. quality-validator checks current branch before commit
3. If on protected branch ‚Üí Migration aborts with error

**You're safe!** The system won't let you commit to protected branches.

---

## Performance Tips

### Speed Up Migrations

1. **Use `/improve` for testing** - No git operations
2. **Migrate small dimensions first** - Test the workflow
3. **Run in parallel** (if multiple dimensions) - Each in separate terminal
4. **Pre-populate TABLE_COUNT.csv** - Faster materialization decisions

### Optimize Snowflake Costs

1. **Use XSMALL warehouse** for development
2. **Limit model selection**: `dbt run --select tag:dimension`
3. **Use views for small tables** (< 10M rows)
4. **Set auto-suspend**: 60 seconds idle time

---

## What's Next?

### After First Migration

1. ‚úÖ Review the Merge Request
2. ‚úÖ Verify models in Snowflake
3. ‚úÖ Run `dbt test` to verify data quality
4. ‚úÖ Deploy custom UDFs (if any)
5. ‚úÖ Merge to main

### Ongoing Usage

- Migrate more dimensions
- Refine variable mappings in `schema_registry.json`
- Update TABLE_COUNT.csv as data grows
- Review and improve generated models

### Optional Enhancements

- Set up GitLab CI/CD for production gate (see `docs/archive/`)
- Add custom dbt macros
- Create data quality tests
- Build data catalog with dbt docs

---

## Getting Help

### Documentation

- **This file:** Getting started guide
- **CLAUDE.md:** Context for Claude Code agents
- **README.md:** Project overview
- **docs/archive/:** Detailed technical documentation

### Check Migration Status

```bash
/migration-status {dimension}
```

Shows:
- Which steps completed
- Current status
- Metadata file locations
- Next steps

### Common Issues

Most issues are auto-fixed or have clear error messages. If stuck:

1. Check validation report: `dimensions/{dimension}/metadata/validation_report.json`
2. Review error in terminal output
3. Check schema_registry.json for missing variables
4. Re-run `/migrate {dimension}` after fixes

---

## Summary

### Key Points

‚úÖ **Two commands:** `/improve` (test) and `/migrate` (production)
‚úÖ **Auto-detects:** GitHub vs GitLab from git remote
‚úÖ **Fast validation:** Local dbt commands (~30 seconds)
‚úÖ **Auto-fix:** Common errors fixed automatically
‚úÖ **No CI/CD needed:** Validates locally (simpler setup)

### Typical Timeline

```
New dimension migration: ~3 minutes
  ‚Ä¢ Parse: 10 sec
  ‚Ä¢ Analyze: 20 sec
  ‚Ä¢ Dependencies: 10 sec
  ‚Ä¢ Translate: 30 sec
  ‚Ä¢ Generate: 30 sec
  ‚Ä¢ Validate (dbt): 30 sec
  ‚Ä¢ Git push: 20 sec
  ‚Ä¢ Total: ~3 min
```

---

**Ready to start?** Run `/migrate dim_date` to test with a small dimension! üöÄ

---

**Version:** 3.1 (Learning System + Local Validation)
**Last Updated:** 2025-10-29
**Complexity:** Low-Medium

# Proyecto de MigraciÃ³n Pentaho a DBT

Sistema automatizado de migraciÃ³n para convertir transformaciones de Pentaho Data Integration (Kettle) en modelos DBT listos para producciÃ³n.

---

## ğŸš€ Inicio RÃ¡pido

### Probar Mejoras Localmente (AHORA)

```bash
# Probar mejoras sin operaciones git
/improve dim_approval_level
```

Crea modelos de prueba en `tfses-dbt-snowflake-3030-ai/` para comparaciÃ³n.

### MigraciÃ³n de ProducciÃ³n (DespuÃ©s de GitHub)

```bash
# MigraciÃ³n completa con operaciones git y CI/CD
/migrate dim_approval_level
```

Crea rama, hace commit, push y espera validaciÃ³n CI/CD.

### Verificar Estado de MigraciÃ³n

```bash
/migration-status dim_approval_level
```

---

## ğŸ“ Estructura del Proyecto

```
3030-pentaho-dbt/
â”œâ”€â”€ .claude/                           # ConfiguraciÃ³n de Claude Code
â”‚   â”œâ”€â”€ agents/                        # Agentes IA para anÃ¡lisis y generaciÃ³n
â”‚   â”œâ”€â”€ commands/                      # Comandos personalizados (/migrate, /improve)
â”‚   â””â”€â”€ skills/                        # Operaciones determinÃ­sticas y plantillas
â”œâ”€â”€ config/                            # Archivos de configuraciÃ³n
â”‚   â”œâ”€â”€ schema_registry.json          # Mapeo de variables y definiciones UDF
â”‚   â”œâ”€â”€ TABLE_COUNT.csv               # Conteo de filas para optimizaciÃ³n
â”‚   â””â”€â”€ tables_columns_info.csv       # Metadata de columnas
â”œâ”€â”€ dimensions/                        # Metadata de migraciÃ³n por dimensiÃ³n
â”‚   â””â”€â”€ <dimension>/
â”‚       â”œâ”€â”€ metadata/                  # Resultados de anÃ¡lisis JSON
â”‚       â””â”€â”€ sql/                       # Archivos SQL traducidos
â”œâ”€â”€ docs/                              # DocumentaciÃ³n
â”‚   â””â”€â”€ GITHUB_CICD_WORKFLOW.md      # GuÃ­a de configuraciÃ³n CI/CD
â”œâ”€â”€ pentaho-sources/                           # ENTRADA: Archivos fuente Pentaho
â”‚   â””â”€â”€ <dimension>/
â”‚       â”œâ”€â”€ *.ktr                     # Transformaciones
â”‚       â””â”€â”€ *.kjb                     # Jobs
â”œâ”€â”€ tfses-dbt-snowflake-3030/         # SALIDA: Repositorio DBT de producciÃ³n
â”‚   â””â”€â”€ models/                        # Modelos DBT generados
â”œâ”€â”€ archive/                           # Archivos antiguos archivados (limpieza)
â””â”€â”€ Archivos de documentaciÃ³n principales
    â”œâ”€â”€ README.md                      # Este archivo
    â”œâ”€â”€ README_ES.md                   # VersiÃ³n en espaÃ±ol
    â”œâ”€â”€ CLAUDE.md                      # Contexto para Claude Code
    â”œâ”€â”€ MIGRATION_WORKFLOW.md          # Flujo de trabajo detallado
    â””â”€â”€ SYSTEM_OVERVIEW.md             # Vista general de la arquitectura
```

---

## ğŸ“– DocumentaciÃ³n

- **[GuÃ­a del Flujo de MigraciÃ³n](MIGRATION_WORKFLOW.md)** - GuÃ­a completa paso a paso
- **[Vista General del Sistema](SYSTEM_OVERVIEW_ES.md)** - Arquitectura y componentes
- **[Contexto de Claude](CLAUDE.md)** - Contexto para Claude Code
- **[GuÃ­a CI/CD de GitHub](docs/GITHUB_CICD_WORKFLOW.md)** - Instrucciones de configuraciÃ³n CI/CD

---

## ğŸ—ï¸ Arquitectura del Sistema

### Dos Flujos de Trabajo de MigraciÃ³n

| Comando | PropÃ³sito | Operaciones Git | UbicaciÃ³n Salida | CuÃ¡ndo Usar |
|---------|-----------|-----------------|------------------|-------------|
| `/improve` | Probar mejoras localmente | âŒ No | `tfses-dbt-snowflake-3030-ai/` | Antes de confirmar cambios |
| `/migrate` | MigraciÃ³n de producciÃ³n | âœ… SÃ­ | `tfses-dbt-snowflake-3030/` | DespuÃ©s de configurar GitHub |

### Flujo del Pipeline (7 Pasos)

```
Paso 0:   ConfiguraciÃ³n Git (crear rama o copiar repo)    â† Â¡NUEVO!
Paso 0.5: AnÃ¡lisis del Repositorio (escanear existente)   â† Â¡NUEVO!
Paso 1:   Parsear Archivos Pentaho
Paso 2:   Analizar Transformaciones
Paso 3:   Construir Dependencias
Paso 4:   Traducir SQL
Paso 5:   Generar Modelos DBT
Paso 6:   Validar y Push (si /migrate)
```

### Relaciones entre Agentes

```
                    repo-analyzer (Â¡NUEVO!)
                         â†“
                [Crea archivos de contexto]
                         â†“
pentaho-parser â†’ pentaho-analyzer â†’ dependency-graph-builder
       â†“                â†“                      â†“
              sql-translator (lee todo)
                         â†“
            dbt-model-generator (evita duplicados)
                         â†“
            quality-validator (ops git si /migrate)
```

### Componentes

**Agentes del Flujo Principal** (Razonamiento impulsado por IA):
1. `repo-analyzer` - Escanea repo DBT, identifica modelos compartidos â† Â¡NUEVO!
2. `pentaho-analyzer` - Resuelve variables, clasifica tablas
3. `dependency-graph-builder` - Detecta dependencias circulares
4. `sql-translator` - Oracle â†’ Snowflake con expansiÃ³n UDF
5. `dbt-model-generator` - Crea modelos, omite compartidos existentes
6. `quality-validator` - ValidaciÃ³n estÃ¡tica + manejo git/CI/CD

**Agentes Auxiliares** (Solucionadores de problemas - bajo demanda):
- `dependency-resolver` - Corrige dependencias circulares
- `pentaho-deep-analyzer` - AnÃ¡lisis profundo de XML Pentaho
- `pentaho-cross-reference` - Encuentra patrones similares
- `sql-function-lookup` - Investiga funciones desconocidas
- `dbt-validator-fixer` - Auto-corrige errores DBT

Ver [docs/HELPER_AGENTS.md](docs/HELPER_AGENTS.md) para cuÃ¡ndo usar agentes auxiliares.

**Skills** (Operaciones determinÃ­sticas):
- `pentaho-parser` - Extrae metadata de XML Pentaho
- `oracle-snowflake-rules` - Patrones de traducciÃ³n SQL
- `dbt-best-practices` - Plantillas y convenciones de nombres

**Comandos** (Orquestadores de flujo de trabajo):
- `/improve <dimension>` - Prueba mejoras localmente
- `/migrate <dimension>` - MigraciÃ³n de producciÃ³n con git
- `/migration-status [dimension]` - Verificar progreso

---

## ğŸ¯ Prerrequisitos

### Requeridos
- Claude Code instalado
- Python 3.8+ (para skill pentaho-parser)

### Opcionales (para validaciÃ³n completa)
- DBT instalado localmente
- ConexiÃ³n a Snowflake configurada
- Cuenta GitHub (para CI/CD)

---

## ğŸ”§ ConfiguraciÃ³n

### 1. Clonar Repositorio DBT (para /migrate)

```bash
git clone https://github.com/tu-org/tfses-dbt-snowflake-3030.git
```

### 2. Configurar Mapeo de Variables

Editar `config/schema_registry.json`:

```json
{
  "variables": {
    "EKIP_SCHEMA": {
      "snowflake_name": "EKIP",
      "type": "external",
      "layer": "bronze"
    }
  }
}
```

### 3. Agregar Archivos Pentaho

Colocar archivos `.ktr` y `.kjb` en `pentaho-sources/<dimension>/`

---

## ğŸƒ Flujo de Trabajo de MigraciÃ³n

### MigraciÃ³n AutomÃ¡tica

```bash
/migrate dim_approval_level
```

Ejecuta el pipeline completo de 6 pasos automÃ¡ticamente.

### EjecuciÃ³n Manual Paso a Paso

Ejecutar cada paso individualmente para mÃ¡s control:

```bash
# Paso 1: Parsear
/pentaho-parser dim_approval_level

# Paso 2: Analizar
[Pedir a Claude ejecutar agente pentaho-analyzer]

# Paso 3: Construir grÃ¡fico de dependencias
[Pedir a Claude ejecutar agente dependency-graph-builder]

# Paso 4: Traducir SQL
[Pedir a Claude ejecutar agente sql-translator]

# Paso 5: Generar modelos DBT
[Pedir a Claude ejecutar agente dbt-model-generator]

# Paso 6: Validar
[Pedir a Claude ejecutar agente quality-validator]
```

Ver [MIGRATION_WORKFLOW.md](MIGRATION_WORKFLOW.md) para instrucciones detalladas.

---

## âœ… ValidaciÃ³n y Pruebas

DespuÃ©s de que la migraciÃ³n complete:

### 1. Verificar Estado de ValidaciÃ³n

```bash
/migration-status dim_approval_level
```

### 2. Revisar Reporte de ValidaciÃ³n

```bash
cat dimensions/dim_approval_level/metadata/validation_report.json | jq
```

### 3. Para /migrate: Esperar CI/CD

DespuÃ©s del push, el sistema mostrarÃ¡:

```
ğŸ”„ GitHub Actions CI/CD estÃ¡ ejecutÃ¡ndose...

ğŸ“‹ PrÃ³ximos pasos:
1. Esperar 2-5 minutos para que CI/CD complete
2. Dime el resultado:
   - Di 'CI passed' si todos los checks âœ…
   - Di 'CI failed' si los checks fallan âŒ

Â¡ManejarÃ© cualquier error automÃ¡ticamente!
```

### 4. Para /improve: Comparar Resultados

```bash
# ComparaciÃ³n visual (VSCode)
code --diff tfses-dbt-snowflake-3030 tfses-dbt-snowflake-3030-ai

# ComparaciÃ³n lÃ­nea de comandos
diff -r tfses-dbt-snowflake-3030/models tfses-dbt-snowflake-3030-ai/models
```

---

## ğŸ“Š Archivos de Salida

### Metadata (Por DimensiÃ³n)

Ubicada en `dimensions/<dimension>/metadata/`:

| Archivo | Fuente | Contiene |
|---------|--------|----------|
| `pentaho_raw.json` | pentaho-parser | Metadata Pentaho parseada |
| `pentaho_analyzed.json` | pentaho-analyzer | ResoluciÃ³n de variables, complejidad |
| `dependency_graph.json` | dependency-graph-builder | Dependencias, orden de ejecuciÃ³n |
| `translation_metadata.json` | sql-translator | Detalles de traducciÃ³n SQL |
| `dbt_generation_report.json` | dbt-model-generator | Resumen de modelos generados |
| `validation_report.json` | quality-validator | Resultados de validaciÃ³n de calidad |

### Modelos DBT

Ubicados en `tfses-dbt-snowflake-3030/models/` (o `-ai/` para /improve):

| Capa | Directorio | PatrÃ³n de Archivo | Ejemplo |
|------|------------|-------------------|---------|
| Bronze | `bronze/` | `_sources.yml` | Definiciones de fuentes |
| Silver ADQ | `silver/silver_adq/` | `stg_*.sql` | `stg_contracts.sql` |
| Silver MAS | `silver/silver_mas/` | `mas_*.sql` | `mas_contracts.sql` |
| Gold | `gold/` | `d_*.sql`, `f_*.sql` | `d_approval_level.sql` |

---

## ğŸš¨ SoluciÃ³n de Problemas

### Errores Comunes

| Error | Causa | SoluciÃ³n |
|-------|-------|----------|
| "Variable no encontrada" | Variable no en registry | Agregar a `schema_registry.json` |
| "Dependencia circular" | Ciclo en transformaciones | Usar agente `dependency-resolver` |
| "FunciÃ³n desconocida" | UDF personalizada | Agregar a `schema_registry.json` |
| "CI/CD falla" | Error en modelo DBT | Decir "CI failed", agente lo arreglarÃ¡ |

### Agentes Auxiliares

Cuando el flujo principal encuentra problemas:

```bash
# Ejemplo: FunciÃ³n SQL desconocida
Task(
    subagent_type="sql-function-lookup",
    prompt="Investigar funciÃ³n CUSTOM_CALC"
)
```

Ver [docs/HELPER_AGENTS.md](docs/HELPER_AGENTS.md) para lista completa.

---

## ğŸ“ˆ MÃ©tricas de Ã‰xito

- âœ… 100% variables resueltas
- âœ… 100% modelos documentados
- âœ… 100% cobertura de pruebas
- âœ… ValidaciÃ³n estÃ¡tica PASSED
- âœ… CI/CD todos los checks verdes

---

## ğŸ¤ Contribuir

1. Crear rama desde `main`
2. Hacer cambios
3. Probar con `/improve`
4. Crear PR cuando estÃ© listo

---

## ğŸ“„ Licencia

Proyecto interno - Equipo de IngenierÃ­a de Datos

---

**VersiÃ³n**: 2.0
**Actualizado**: 2025-10-25
**Mantenido por**: Equipo de IngenierÃ­a de Datos
# GuÃ­a de Inicio - MigraciÃ³n de Pentaho a DBT

GuÃ­a completa para configurar y ejecutar el sistema automatizado de migraciÃ³n de Pentaho a DBT.

---

## ðŸš¨ REGLA MEGA-IMPORTANTE

**Â¡NUNCA hacer commit directamente a las ramas `develop`, `master`, o `main`!**

El sistema tiene protecciÃ³n integrada para prevenir esto, pero siempre debes:
- âœ… Trabajar en ramas de caracterÃ­sticas (`migrate/nombre_dimension`)
- âœ… Crear Pull/Merge Requests para revisiÃ³n de cÃ³digo
- âŒ NUNCA hacer commit directamente a ramas protegidas

**El comando `/migrate` crea ramas de caracterÃ­sticas automÃ¡ticamente.**

---

## Inicio RÃ¡pido (5 Minutos)

### 1. Requisitos Previos

âœ… **Ya instalados:**
- Git Bash (MINGW64)
- DBT Cloud CLI (`dbt.exe` en `tfses-dbt-snowflake-3030/`)
- ConexiÃ³n a Snowflake configurada

âœ… **Necesitas instalar:**
```bash
# GitLab CLI (si usas GitLab)
winget install gitlab.glab
glab auth login

# GitHub CLI (si usas GitHub)
winget install GitHub.cli
gh auth login
```

### 2. ConfiguraciÃ³n Ãšnica (PATH)

Ejecuta este script una vez para hacer disponible el comando `dbt` en todas partes:

```bash
cd 3030-pentaho-dbt
bash setup-dbt-path.sh
source ~/.bashrc
```

**Verifica que funcionÃ³:**
```bash
dbt --version
# DeberÃ­a mostrar: dbt Cloud CLI - 0.40.7
```

### 3. Ejecuta tu Primera MigraciÃ³n

```bash
# Prueba con dimensiÃ³n pequeÃ±a (seguro, sin git)
/improve dim_date

# MigraciÃ³n a producciÃ³n (con git push)
/migrate dim_approval_level
```

**Â¡Listo!** âœ…

---

## Comprendiendo el Sistema

### QuÃ© Hace

Convierte automÃ¡ticamente **transformaciones de Pentaho** (.ktr, .kjb) en **modelos DBT listos para producciÃ³n** en Snowflake:

```
XML Pentaho â†’ Parsear â†’ Analizar â†’ Traducir â†’ Generar â†’ Validar â†’ Git Push
```

### Dos Comandos

| Comando | PropÃ³sito | Operaciones Git | CuÃ¡ndo Usar |
|---------|-----------|-----------------|-------------|
| `/improve` | Prueba local | âŒ No | Probar mejoras, experimentaciÃ³n segura |
| `/migrate` | ProducciÃ³n | âœ… SÃ­ | Listo para desplegar a producciÃ³n |

---

## Flujo de MigraciÃ³n

### Proceso Paso a Paso

**1. Parsear** (`pentaho-parser` skill)
```bash
Entrada:  pentaho-sources/dim_approval_level/*.ktr, *.kjb
Salida:   dimensions/dim_approval_level/metadata/pentaho_raw.json
```
Extrae SQL, variables, pasos, tablas del XML de Pentaho.

**2. Analizar** (`pentaho-analyzer` agent)
```bash
Entrada:  pentaho_raw.json, schema_registry.json
Salida:   pentaho_analyzed.json
```
Resuelve variables, clasifica tablas (bronze/silver/gold), evalÃºa complejidad.

**3. Construir Dependencias** (`dependency-graph-builder` agent)
```bash
Entrada:  pentaho_raw.json, pentaho_analyzed.json
Salida:   dependency_graph.json, dependency_graph.mmd
```
Determina orden de ejecuciÃ³n, detecta dependencias circulares.

**4. Traducir SQL** (`sql-translator` agent)
```bash
Entrada:  pentaho_analyzed.json, oracle-snowflake-rules
Salida:   *_translated.sql, translation_metadata.json
```
Convierte SQL de Oracle a Snowflake, preserva UDFs personalizados.

**5. Generar Modelos DBT** (`dbt-model-generator` agent)
```bash
Entrada:  translation_metadata.json, dbt-best-practices
Salida:   Modelos DBT en models/silver/, models/gold/
```
Crea modelos DBT listos para producciÃ³n con documentaciÃ³n y tests.

**6. Validar Localmente** (`quality-validator` agent) âœ¨ **Â¡NUEVO!**
```bash
Ejecuta LOCALMENTE (sin esperar CI/CD):
  â€¢ dbt parse   (validaciÃ³n de sintaxis)
  â€¢ dbt compile (validaciÃ³n de templates)
  â€¢ dbt run     (crear modelos en Snowflake)
  â€¢ dbt test    (tests de calidad de datos)

Si hay errores: Auto-correcciÃ³n y reintento (mÃ¡x 2 veces)
Si pasa: Git commit + push
```

**Tiempo Total:** ~3 minutos (antes eran 10-15 min con CI/CD)

---

## Soporte de Plataformas (GitHub & GitLab)

El sistema **auto-detecta** tu plataforma Git desde la URL remota:

### GitHub
```bash
Remoto: https://github.com/org/repo.git
â†’ Detectado: GitHub
â†’ Usa: gh CLI
â†’ Crea: Pull Request
```

### GitLab
```bash
Remoto: https://gitlab.com/org/repo.git
â†’ Detectado: GitLab
â†’ Usa: glab CLI
â†’ Crea: Merge Request
```

**Â¡El mismo comando `/migrate` funciona para ambos!**

---

## Archivos de ConfiguraciÃ³n

### schema_registry.json

Mapea variables de Pentaho a esquemas de Snowflake:

```json
{
  "variables": {
    "EKIP_SCHEMA": {
      "snowflake_name": "EKIP",
      "type": "external",
      "layer": "bronze"
    }
  },
  "custom_functions": [
    {
      "name": "GETENNUML",
      "preserve": true,
      "deployment_required": true
    }
  ]
}
```

**CuÃ¡ndo editar:**
- Agregar nueva variable de Pentaho
- Declarar UDF personalizado (para que no se traduzca)

### TABLE_COUNT.csv (Opcional)

Usado para optimizaciÃ³n de materializaciÃ³n:

```csv
schema,table,row_count
EKIP,CONTRACTS,50234
EKIP,CUSTOMERS,12500
```

**Reglas:**
- `> 10M filas` â†’ Materializado como `table`
- `< 10M filas` â†’ Materializado como `view`

---

## Estructura de Carpetas

```
3030-pentaho-dbt/
â”œâ”€â”€ CLAUDE.md                    # Contexto para Claude Code
â”œâ”€â”€ README.md                    # Resumen (inglÃ©s)
â”œâ”€â”€ README_ES.md                 # Resumen (espaÃ±ol)
â”œâ”€â”€ GETTING_STARTED.md          # Esta guÃ­a (inglÃ©s)
â”œâ”€â”€ GETTING_STARTED_ES.md       # Esta guÃ­a (espaÃ±ol)
â”œâ”€â”€ setup-dbt-path.sh           # Script configuraciÃ³n PATH
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ schema_registry.json    # Mapeos de variables
â”‚   â””â”€â”€ TABLE_COUNT.csv         # TamaÃ±os de tablas (opcional)
â”‚
â”œâ”€â”€ pentaho-sources/                    # ENTRADA: Archivos fuente Pentaho
â”‚   â””â”€â”€ dim_approval_level/
â”‚       â”œâ”€â”€ d_approval_level.ktr
â”‚       â””â”€â”€ *.kjb
â”‚
â”œâ”€â”€ dimensions/                 # SALIDA: Metadatos por dimensiÃ³n
â”‚   â””â”€â”€ dim_approval_level/
â”‚       â”œâ”€â”€ metadata/
â”‚       â”‚   â”œâ”€â”€ pentaho_raw.json
â”‚       â”‚   â”œâ”€â”€ pentaho_analyzed.json
â”‚       â”‚   â”œâ”€â”€ dependency_graph.json
â”‚       â”‚   â”œâ”€â”€ translation_metadata.json
â”‚       â”‚   â”œâ”€â”€ dbt_generation_report.json
â”‚       â”‚   â””â”€â”€ validation_report.json
â”‚       â””â”€â”€ sql/
â”‚           â””â”€â”€ *_translated.sql
â”‚
â”œâ”€â”€ tfses-dbt-snowflake-3030/  # Repositorio DBT (git)
â”‚   â”œâ”€â”€ dbt.exe                 # Binario DBT CLI
â”‚   â”œâ”€â”€ profiles.yml            # ConexiÃ³n Snowflake
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ bronze/_sources.yml
â”‚       â”œâ”€â”€ silver/
â”‚       â”‚   â”œâ”€â”€ silver_adq/
â”‚       â”‚   â””â”€â”€ silver_mas/
â”‚       â””â”€â”€ gold/
â”‚
â””â”€â”€ .claude/
    â”œâ”€â”€ skills/                 # Operaciones determinÃ­sticas
    â”œâ”€â”€ agents/                 # AnÃ¡lisis con IA
    â””â”€â”€ commands/               # OrquestaciÃ³n de flujo
```

---

## Convenciones de Nombres DBT

El sistema sigue reglas estrictas de nombres:

| Archivo Pentaho | Modelo DBT | Capa |
|-----------------|------------|------|
| `adq_ekip_contracts.ktr` | `silver/silver_adq/stg_ekip_contracts.sql` | Silver ADQ |
| `mas_contracts.kjb` | `silver/silver_mas/mas_contracts.sql` | Silver MAS |
| `d_approval_level.ktr` | `gold/d_approval_level.sql` | Gold (dimensiÃ³n) |
| `f_sales.ktr` | `gold/f_sales.sql` | Gold (hecho) |

**PatrÃ³n:**
- Quitar prefijo `adq_` â†’ Agregar prefijo `stg_`
- Mantener prefijo `mas_`
- Mantener prefijos `d_` (dimensiÃ³n) y `f_` (hecho)

---

## Tareas Comunes

### Migrar una Nueva DimensiÃ³n

```bash
# 1. Colocar archivos Pentaho
mkdir pentaho-sources/dim_customer
cp /path/to/*.ktr pentaho-sources/dim_customer/

# 2. Ejecutar migraciÃ³n
/migrate dim_customer

# 3. Revisar reporte de validaciÃ³n
cat dimensions/dim_customer/metadata/validation_report.json | jq

# 4. Revisar Merge Request y fusionar
```

### Probar Mejoras Antes de Desplegar

```bash
# Hacer cambios a agents/skills
# Probar sin operaciones git
/improve dim_customer

# Comparar resultados
diff -r tfses-dbt-snowflake-3030/models tfses-dbt-snowflake-3030-ai/models

# Si es bueno, ejecutar producciÃ³n
/migrate dim_customer
```

### Agregar Nuevo Mapeo de Variable

Editar `config/schema_registry.json`:

```json
{
  "variables": {
    "NUEVO_SCHEMA": {
      "snowflake_name": "NOMBRE_SCHEMA_REAL",
      "type": "external",
      "layer": "bronze"
    }
  }
}
```

Luego re-ejecutar `/migrate`.

### Declarar UDF Personalizado

Editar `config/schema_registry.json`:

```json
{
  "custom_functions": [
    {
      "name": "MI_FUNCION_PERSONALIZADA",
      "preserve": true,
      "deployment_required": true,
      "description": "UDF personalizado - no traducir"
    }
  ]
}
```

**Recuerda:** Â¡Desplegar UDF a Snowflake antes de ejecutar modelos!

---

## SoluciÃ³n de Problemas

### "dbt: command not found"

**Causa:** PATH no configurado

**SoluciÃ³n:**
```bash
source ~/.bashrc
# o
bash setup-dbt-path.sh
```

### "Cannot connect to Snowflake"

**Causa:** profiles.yml mal configurado

**SoluciÃ³n:**
```bash
cd tfses-dbt-snowflake-3030
dbt debug  # Probar conexiÃ³n
```

Verificar que `profiles.yml` tenga credenciales correctas.

### Error "Variable not found"

**Causa:** Variable de Pentaho no estÃ¡ en schema_registry.json

**SoluciÃ³n:**
Agregar a `config/schema_registry.json`:
```json
{
  "variables": {
    "TU_SCHEMA": {
      "snowflake_name": "NOMBRE_REAL",
      "type": "external",
      "layer": "bronze"
    }
  }
}
```

### "Circular dependency detected"

**Causa:** Transformaciones de Pentaho dependen entre sÃ­ en un ciclo

**SoluciÃ³n:**
1. Revisar `dimensions/{dimension}/metadata/dependency_graph.mmd`
2. Identificar el ciclo
3. RediseÃ±ar lÃ³gica de transformaciÃ³n para romper el ciclo
4. Ver `dependency_graph.json` para puntos de ruptura sugeridos

### MigraciÃ³n Falla con Errores

**El sistema:**
1. IntentarÃ¡ auto-correcciÃ³n (fuentes faltantes, etc.) - mÃ¡x 2 veces
2. Si no puede corregir: MostrarÃ¡ mensaje de error claro con pasos de remediaciÃ³n
3. Corregir manualmente
4. Re-ejecutar `/migrate {dimension}`

---

## Referencia de Comandos

### Comandos de MigraciÃ³n

```bash
/migrate {dimension}          # MigraciÃ³n completa con git push
/improve {dimension}          # Prueba local (sin git)
/migration-status             # Ver todas las dimensiones
/migration-status {dimension} # Ver dimensiÃ³n especÃ­fica
```

### Comandos DBT (Local)

```bash
cd tfses-dbt-snowflake-3030

# ValidaciÃ³n
dbt parse                              # Verificar sintaxis
dbt compile                            # Resolver templates
dbt debug                              # Probar conexiÃ³n Snowflake

# EjecuciÃ³n
dbt run                                # Ejecutar todos los modelos
dbt run --select tag:dim_customer      # Ejecutar dimensiÃ³n especÃ­fica
dbt test                               # Ejecutar todos los tests
dbt test --select tag:dim_customer     # Testear dimensiÃ³n especÃ­fica

# DocumentaciÃ³n
dbt docs generate                      # Generar documentaciÃ³n
dbt docs serve                         # Ver documentaciÃ³n en navegador
```

### Comandos Git (si es necesario)

```bash
cd tfses-dbt-snowflake-3030

# Ver estado
git status
git branch  # Ver rama actual
git log --oneline -10

# Crear MR/PR manualmente
glab mr create  # GitLab
gh pr create    # GitHub

# Ver MR/PR
glab mr view    # GitLab
gh pr view      # GitHub
```

### ðŸš¨ Seguridad de Ramas

**CRÃTICO:** El sistema hace cumplir esta regla automÃ¡ticamente:

```bash
# âœ… BIEN - Rama de caracterÃ­stica
git checkout -b migrate/dim_customer

# âŒ MAL - Rama protegida (Â¡BLOQUEADO!)
git checkout develop  # Sistema abortarÃ¡ la migraciÃ³n
git checkout master   # Sistema abortarÃ¡ la migraciÃ³n
git checkout main     # Sistema abortarÃ¡ la migraciÃ³n
```

**ProtecciÃ³n implementada:**
1. El comando `/migrate` crea rama de caracterÃ­stica automÃ¡ticamente
2. quality-validator verifica rama actual antes de hacer commit
3. Si estÃ¡ en rama protegida â†’ MigraciÃ³n se aborta con error

**Â¡EstÃ¡s seguro!** El sistema no te permitirÃ¡ hacer commit a ramas protegidas.

---

## Consejos de Rendimiento

### Acelerar Migraciones

1. **Usar `/improve` para pruebas** - Sin operaciones git
2. **Migrar dimensiones pequeÃ±as primero** - Probar el flujo
3. **Ejecutar en paralelo** (si mÃºltiples dimensiones) - Cada una en terminal separada
4. **Pre-poblar TABLE_COUNT.csv** - Decisiones de materializaciÃ³n mÃ¡s rÃ¡pidas

### Optimizar Costos de Snowflake

1. **Usar warehouse XSMALL** para desarrollo
2. **Limitar selecciÃ³n de modelos**: `dbt run --select tag:dimension`
3. **Usar vistas para tablas pequeÃ±as** (< 10M filas)
4. **Configurar auto-suspend**: 60 segundos de inactividad

---

## Â¿QuÃ© Sigue?

### DespuÃ©s de la Primera MigraciÃ³n

1. âœ… Revisar el Merge Request
2. âœ… Verificar modelos en Snowflake
3. âœ… Ejecutar `dbt test` para verificar calidad de datos
4. âœ… Desplegar UDFs personalizados (si hay)
5. âœ… Fusionar a main

### Uso Continuo

- Migrar mÃ¡s dimensiones
- Refinar mapeos de variables en `schema_registry.json`
- Actualizar TABLE_COUNT.csv conforme crecen los datos
- Revisar y mejorar modelos generados

---

## Obtener Ayuda

### DocumentaciÃ³n

- **Este archivo:** GuÃ­a de inicio
- **CLAUDE.md:** Contexto para agentes de Claude Code
- **README_ES.md:** Resumen del proyecto
- **docs/archive/:** DocumentaciÃ³n tÃ©cnica detallada

### Verificar Estado de MigraciÃ³n

```bash
/migration-status {dimension}
```

Muestra:
- QuÃ© pasos se completaron
- Estado actual
- Ubicaciones de archivos de metadatos
- PrÃ³ximos pasos

---

## Resumen

### Puntos Clave

âœ… **Dos comandos:** `/improve` (prueba) y `/migrate` (producciÃ³n)
âœ… **Auto-detecta:** GitHub vs GitLab desde git remote
âœ… **ValidaciÃ³n rÃ¡pida:** Comandos dbt locales (~30 segundos)
âœ… **Auto-correcciÃ³n:** Errores comunes corregidos automÃ¡ticamente
âœ… **No necesita CI/CD:** Valida localmente (configuraciÃ³n mÃ¡s simple)

### LÃ­nea de Tiempo TÃ­pica

```
MigraciÃ³n nueva dimensiÃ³n: ~3 minutos
  â€¢ Parsear: 10 seg
  â€¢ Analizar: 20 seg
  â€¢ Dependencias: 10 seg
  â€¢ Traducir: 30 seg
  â€¢ Generar: 30 seg
  â€¢ Validar (dbt): 30 seg
  â€¢ Git push: 20 seg
  â€¢ Total: ~3 min
```

---

**Â¿Listo para comenzar?** Â¡Ejecuta `/migrate dim_date` para probar con una dimensiÃ³n pequeÃ±a! ðŸš€

---

**VersiÃ³n:** 3.0 (ValidaciÃ³n Local)
**Ãšltima ActualizaciÃ³n:** 2025-01-27
**Complejidad:** Baja-Media
